{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divorce predictor\n",
    "The data is from: https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set, project created for #30Days10AIprojects by: https://doingai.tech/2020/01/30days10aiprojects/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=170, n=54, Y=(170,)\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "X = np.array(pd.read_csv('data.csv', sep=';'))\n",
    "Y = X[:, 54] # Classes \n",
    "\n",
    "X = np.delete(X, 54, 1)\n",
    "\n",
    "# Number of training examples\n",
    "m = X.shape[0]\n",
    "# Number of features\n",
    "n = X.shape[1]\n",
    "\n",
    "print(f'm={m}, n={n}, Y={Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network Architecture\n",
    "The model is trained with a Deep Neural Network with 3 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "class Model():\n",
    "    def __init__(self, n, n_epochs = 10):\n",
    "        self.parameters = {}\n",
    "        self.n_hidden_layers = 3\n",
    "        self.n_neurons = [54, 4, 3, 1]\n",
    "        self.gradients = {}\n",
    "        self.n_epochs = n_epochs\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        for i in range(1, self.n_hidden_layers + 1):\n",
    "            self.parameters[\"W\"+str(i)] = np.random.randn(self.n_neurons[i-1], self.n_neurons[i]) * np.sqrt(1 / self.n_neurons[i-1])\n",
    "            self.parameters[\"B\"+str(i)] = np.zeros([1, self.n_neurons[i]])\n",
    "        \n",
    "    # Activation for output layer    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_backward(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    # Activation for neurons\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    def relu_backward(self, x):\n",
    "        return 1.0 * (x > 0)\n",
    "    \n",
    "    # Cost function\n",
    "    def cost(self, Y, y_hat):\n",
    "        m = Y.shape[0]\n",
    "        result = np.sum((Y * np.log(y_hat) + (1 - Y) * np.log(1 - y_hat))) / -m\n",
    "        result = np.squeeze(result)\n",
    "        return result\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward_propagation(self, X, parameters = None):\n",
    "        if parameters is None:\n",
    "            parameters = self.parameters\n",
    "            \n",
    "        self.A1 = np.dot(X, parameters[\"W1\"]) + parameters[\"B1\"]\n",
    "        self.Z1 = self.relu(self.A1)\n",
    "        \n",
    "        self.A2 = np.dot(self.Z1, parameters[\"W2\"]) + parameters[\"B2\"]\n",
    "        self.Z2 = self.relu(self.A2)\n",
    "        \n",
    "        self.A3 = np.dot(self.Z2, parameters[\"W3\"]) + parameters[\"B3\"]\n",
    "        self.y_hat = self.sigmoid(self.A3)\n",
    "        return self.y_hat\n",
    "    \n",
    "    # Backward propagation\n",
    "    def backward_propagation(self, X, Y, parameters = None):\n",
    "        if parameters is None:\n",
    "            parameters = self.parameters \n",
    "      \n",
    "        self.forward_propagation(X, parameters)\n",
    "        \n",
    "        self.gradients[\"dA3\"] = self.y_hat - Y\n",
    "        self.gradients[\"dW3\"] = np.dot(self.Z2.T, self.gradients[\"dA3\"])\n",
    "        self.gradients[\"dB3\"] = np.sum(self.gradients[\"dA3\"], axis=0).reshape(1, -1)\n",
    "        \n",
    "        self.gradients[\"dZ2\"] = np.dot(self.gradients[\"dA3\"], parameters[\"W3\"].T)\n",
    "        self.gradients[\"dA2\"] = np.multiply(self.gradients[\"dZ2\"], self.relu_backward(self.Z2))\n",
    "        self.gradients[\"dW2\"] = np.dot(self.Z1.T, self.gradients[\"dA2\"])\n",
    "        self.gradients[\"dB2\"] = np.sum(self.gradients[\"dA2\"], axis=0).reshape(1, -1)\n",
    "        \n",
    "        self.gradients[\"dZ1\"] = np.dot(self.gradients[\"dA2\"], parameters[\"W2\"].T)\n",
    "        self.gradients[\"dA1\"] = np.multiply(self.gradients[\"dZ1\"], self.relu_backward(self.Z1))\n",
    "        self.gradients[\"dW1\"] = np.dot(X.T, self.gradients[\"dA1\"])\n",
    "        self.gradients[\"dB1\"] = np.sum(self.gradients[\"dA1\"], axis=0).reshape(1, -1)\n",
    "        \n",
    "    # Gradient descent\n",
    "    def fit(self, X, Y, learning_rate = 0.1):\n",
    "        costs = {}\n",
    "        Y_pred = self.predict(X)\n",
    "        costs[0] = self.cost(np.argmax(Y, axis=1), Y_pred)\n",
    "        \n",
    "        \n",
    "        print(f\"Cost after iteration 0: {costs[0]}\")\n",
    "        for num_epoch in range(self.n_epochs):\n",
    "            m = X.shape[0]\n",
    "            self.backward_propagation(X, Y)\n",
    "            # Parameters update\n",
    "            for i in range(1, self.n_hidden_layers + 1):\n",
    "                self.parameters[\"W\"+str(i)] -= learning_rate * (self.gradients[\"dW\"+str(i)] / m)\n",
    "                self.parameters[\"B\"+str(i)] -= learning_rate * (self.gradients[\"dB\"+str(i)] / m)\n",
    "\n",
    "            # Calculate cost    \n",
    "            Y_pred = self.predict(X)\n",
    "            costs[num_epoch + 1] = self.cost(np.argmax(Y, axis=1), Y_pred)\n",
    "\n",
    "            # Plot cost\n",
    "            print(f\"Cost after iteration {num_epoch + 1}: {costs[num_epoch + 1]}\")\n",
    "    \n",
    "    # Predict labels\n",
    "    def predict(self, X):\n",
    "        prediction = self.forward_propagation(X)\n",
    "        return prediction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Model(n, 119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.30782763387843276\n",
      "Cost after iteration 1: 0.7056250570255529\n",
      "Cost after iteration 2: 0.7055087851954146\n",
      "Cost after iteration 3: 0.705395194953746\n",
      "Cost after iteration 4: 0.7052842130031091\n",
      "Cost after iteration 5: 0.7051757677972126\n",
      "Cost after iteration 6: 0.7050697894926569\n",
      "Cost after iteration 7: 0.704966209901875\n",
      "Cost after iteration 8: 0.7048649624472407\n",
      "Cost after iteration 9: 0.704765982116293\n",
      "Cost after iteration 10: 0.7046690723150812\n",
      "Cost after iteration 11: 0.7045529277457385\n",
      "Cost after iteration 12: 0.7044379131186833\n",
      "Cost after iteration 13: 0.7043302921074259\n",
      "Cost after iteration 14: 0.7042237907247869\n",
      "Cost after iteration 15: 0.7041152424496074\n",
      "Cost after iteration 16: 0.7040127219309423\n",
      "Cost after iteration 17: 0.7039073719374822\n",
      "Cost after iteration 18: 0.7038085154349942\n",
      "Cost after iteration 19: 0.7037055583427185\n",
      "Cost after iteration 20: 0.7036078384706278\n",
      "Cost after iteration 21: 0.7035112978848026\n",
      "Cost after iteration 22: 0.7034099659993149\n",
      "Cost after iteration 23: 0.7033162385570433\n",
      "Cost after iteration 24: 0.7032236391489844\n",
      "Cost after iteration 25: 0.7031319019443368\n",
      "Cost after iteration 26: 0.7030345602005769\n",
      "Cost after iteration 27: 0.7029411472165268\n",
      "Cost after iteration 28: 0.7028442300016406\n",
      "Cost after iteration 29: 0.7027453365324493\n",
      "Cost after iteration 30: 0.7026469983293766\n",
      "Cost after iteration 31: 0.7025491197105005\n",
      "Cost after iteration 32: 0.70245083463453\n",
      "Cost after iteration 33: 0.7023479418304873\n",
      "Cost after iteration 34: 0.7022470476040337\n",
      "Cost after iteration 35: 0.7021440004682229\n",
      "Cost after iteration 36: 0.7020342330948963\n",
      "Cost after iteration 37: 0.7019193243348228\n",
      "Cost after iteration 38: 0.7018039829296151\n",
      "Cost after iteration 39: 0.7016812337245653\n",
      "Cost after iteration 40: 0.7015653313154376\n",
      "Cost after iteration 41: 0.7014483498911\n",
      "Cost after iteration 42: 0.7013317114197136\n",
      "Cost after iteration 43: 0.701213641708164\n",
      "Cost after iteration 44: 0.7010939792210162\n",
      "Cost after iteration 45: 0.7009719194359254\n",
      "Cost after iteration 46: 0.7008362840124954\n",
      "Cost after iteration 47: 0.7006972636836724\n",
      "Cost after iteration 48: 0.7005520220056733\n",
      "Cost after iteration 49: 0.7004023158122704\n",
      "Cost after iteration 50: 0.7002519941065015\n",
      "Cost after iteration 51: 0.700092920365456\n",
      "Cost after iteration 52: 0.6999276308316487\n",
      "Cost after iteration 53: 0.6997577688342317\n",
      "Cost after iteration 54: 0.699578658117724\n",
      "Cost after iteration 55: 0.6993728103697605\n",
      "Cost after iteration 56: 0.6991538781003295\n",
      "Cost after iteration 57: 0.6989256798980412\n",
      "Cost after iteration 58: 0.698690760306961\n",
      "Cost after iteration 59: 0.6984290028294317\n",
      "Cost after iteration 60: 0.6981778698267631\n",
      "Cost after iteration 61: 0.6978871526637848\n",
      "Cost after iteration 62: 0.6975967883739866\n",
      "Cost after iteration 63: 0.6972591919294112\n",
      "Cost after iteration 64: 0.6969351752884341\n",
      "Cost after iteration 65: 0.6965882827612103\n",
      "Cost after iteration 66: 0.6961229626337018\n",
      "Cost after iteration 67: 0.6956487830839089\n",
      "Cost after iteration 68: 0.6951456541687361\n",
      "Cost after iteration 69: 0.69453782614049\n",
      "Cost after iteration 70: 0.6939356656401036\n",
      "Cost after iteration 71: 0.693282101496846\n",
      "Cost after iteration 72: 0.6925882600095389\n",
      "Cost after iteration 73: 0.6918451668924511\n",
      "Cost after iteration 74: 0.6909481479423141\n",
      "Cost after iteration 75: 0.6900406301965377\n",
      "Cost after iteration 76: 0.6890333334954032\n",
      "Cost after iteration 77: 0.6878696961739972\n",
      "Cost after iteration 78: 0.6867044266594616\n",
      "Cost after iteration 79: 0.6853057731480992\n",
      "Cost after iteration 80: 0.6838478075933007\n",
      "Cost after iteration 81: 0.6821911286535591\n",
      "Cost after iteration 82: 0.6803094879852863\n",
      "Cost after iteration 83: 0.6782893146780146\n",
      "Cost after iteration 84: 0.676029966409102\n",
      "Cost after iteration 85: 0.6738926791610017\n",
      "Cost after iteration 86: 0.6714243902204433\n",
      "Cost after iteration 87: 0.6687204586804412\n",
      "Cost after iteration 88: 0.6659193262779785\n",
      "Cost after iteration 89: 0.6630517806564729\n",
      "Cost after iteration 90: 0.6598780256630596\n",
      "Cost after iteration 91: 0.6558024053731959\n",
      "Cost after iteration 92: 0.6514365341326271\n",
      "Cost after iteration 93: 0.647767304593181\n",
      "Cost after iteration 94: 0.6414448036656768\n",
      "Cost after iteration 95: 0.6401514081648261\n",
      "Cost after iteration 96: 0.6324885612817194\n",
      "Cost after iteration 97: 0.6241048317573001\n",
      "Cost after iteration 98: 0.6251871540147874\n",
      "Cost after iteration 99: 0.6145037278474937\n",
      "Cost after iteration 100: 0.6038798082000995\n",
      "Cost after iteration 101: 0.6037886832051997\n",
      "Cost after iteration 102: 0.5917514814441472\n",
      "Cost after iteration 103: 0.5799388074774023\n",
      "Cost after iteration 104: 0.5802724039655908\n",
      "Cost after iteration 105: 0.5665541393929721\n",
      "Cost after iteration 106: 0.5573180155594276\n",
      "Cost after iteration 107: 0.5512577192066235\n",
      "Cost after iteration 108: 0.5416257812156334\n",
      "Cost after iteration 109: 0.537622958920272\n",
      "Cost after iteration 110: 0.5236326010524226\n",
      "Cost after iteration 111: 0.5326325328782395\n",
      "Cost after iteration 112: 0.5211514181722793\n",
      "Cost after iteration 113: 0.5116296602899579\n",
      "Cost after iteration 114: 0.5127215594478268\n",
      "Cost after iteration 115: 0.5036660261827518\n",
      "Cost after iteration 116: 0.5075129545174091\n",
      "Cost after iteration 117: 0.5014211747894782\n",
      "Cost after iteration 118: 0.5021972580303019\n",
      "Cost after iteration 119: 0.4978029930802317\n"
     ]
    }
   ],
   "source": [
    "Y_train = Y_train.reshape(119, 1)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  97.47899159663865 %\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "predictions = classifier.predict(X_train)\n",
    "print(\"Training accuracy: \", accuracy_score(Y_train, predictions > 0.5) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  94.11764705882352 %\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Test accuracy: \", accuracy_score(Y_test, predictions > 0.5) * 100, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
